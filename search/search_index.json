{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"datar-blog A blog about datar Blog: https://pwwang.github.io/datar-blog Blog Repo: https://github.com/pwwang/datar-blog datar: https://github.com/pwwang/datar datar docs: https://pwwang.github.io/datar pipda: https://github.com/pwwang/pipda Table of Contents Porting dplyr to python: Efforts been made Porting dplyr to python: Implementing piping Porting dplyr to python: Verb argument evaluation Showing data types under column names when printing pandas data frames","title":"Home"},{"location":"index.html#datar-blog","text":"A blog about datar Blog: https://pwwang.github.io/datar-blog Blog Repo: https://github.com/pwwang/datar-blog datar: https://github.com/pwwang/datar datar docs: https://pwwang.github.io/datar pipda: https://github.com/pwwang/pipda","title":"datar-blog"},{"location":"index.html#table-of-contents","text":"Porting dplyr to python: Efforts been made Porting dplyr to python: Implementing piping Porting dplyr to python: Verb argument evaluation Showing data types under column names when printing pandas data frames","title":"Table of Contents"},{"location":"efforts.html","text":"dplyr vs pandas Why people have made such efforts to port dplyr to python, include me? Well, one of the biggest reasons is that dplyr has much cleaner APIs. pandas has some comparisons between these two with some simple operations on data frames. See its documentation [here]. Check out the left column of the tables, you may find the almost every statement in R / dplyr is shorter. If things go complicated, say, filter a data frame with values from 2 columns: >>> iris [( iris [ 'Species' ] == 'setosa' ) & ( iris [ 'Petal_Length' ] > 1.5 )] Sepal_Length Sepal_Width Petal_Length Petal_Width Species < float64 > < float64 > < float64 > < float64 > < object > 5 5.4 3.9 1.7 0.4 setosa 11 4.8 3.4 1.6 0.2 setosa 18 5.7 3.8 1.7 0.3 setosa 20 5.4 3.4 1.7 0.2 setosa 23 5.1 3.3 1.7 0.5 setosa 24 4.8 3.4 1.9 0.2 setosa 25 5.0 3.0 1.6 0.2 setosa 26 5.0 3.4 1.6 0.4 setosa 29 4.7 3.2 1.6 0.2 setosa 30 4.8 3.1 1.6 0.2 setosa 43 5.0 3.5 1.6 0.6 setosa 44 5.1 3.8 1.9 0.4 setosa 46 5.1 3.8 1.6 0.2 setosa r $> iris %>% filter ( Species == 'setosa' & Petal.Length > 1.5 ) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.4 3.9 1.7 0.4 setosa 2 4.8 3.4 1.6 0.2 setosa 3 5.7 3.8 1.7 0.3 setosa 4 5.4 3.4 1.7 0.2 setosa 5 5.1 3.3 1.7 0.5 setosa 6 4.8 3.4 1.9 0.2 setosa 7 5.0 3.0 1.6 0.2 setosa 8 5.0 3.4 1.6 0.4 setosa 9 4.7 3.2 1.6 0.2 setosa 10 4.8 3.1 1.6 0.2 setosa 11 5.0 3.5 1.6 0.6 setosa 12 5.1 3.8 1.9 0.4 setosa 13 5.1 3.8 1.6 0.2 setosa What if you have even more columns and operations involved? There are some other in-depth comparisons: - Python\u2019s Pandas vs. R\u2019s dplyr \u2013 Which Is The Best Data Analysis Library Efforts been made In order to turn this in R: iris %>% filter ( Species == 'setosa' ) to this in python: iris >> filter ( Species == 'setosa' ) efforts have been made to defer the execution of filter(Species == 'setosa') . Since it gets evaluated when python sees it right away, but we need the data ( iris ) to be piped in to evaluate it. suiba suiba creates a Pipeable object for the verb, and attach __rshift__ method to the DataFrame class or other types. To defer the evaluation of the arguments in the verbs, suiba creates a sui expression that is initiated with a Symbolic object (named _ by default). suiba is powerful and potential to port more related functionalities from R packages, and has great support fro remote tables. It is also attracting attention in the community. suiba has ported a couple verbs from dplyr , including: select() - keep certain columns of data. filter() - keep certain rows of data. mutate() - create or modify an existing column of data. summarize() - reduce one or more columns down to a single number. arrange() - reorder the rows of data. that are most commonly used in data processing. But not all verbs and other helper functions are ported. dfply dfply wraps verbs by a pipe decorator to turn them into a pipeable object (with __rshift__() and __rrshift__() defined). The arguments of the verbs are pre-compiled as Intention objects. Later on the data is piped in and the Intention objects get evaluated. plydata plydata has very similar ideas as dfply does. It uses metaclass to make verbs pipeable. Instead of pre-compile the arguments into objects, plydata uses raw strings as the arguments. The magic for evaluation is to capture the environment from call stacks, by EvalEnvironment.capture() . dplython dplython subclasses pandas ' DataFrame class to DplyFrame , which enables piping ( __rshift__ ) and some grouping functions by itself to easily implement dplyr 's group_by and related functions. The Later objects have carried the information for the function to be evaluated later. The wrapper around it DelayedFunction enables external functions to be registered as verbs. Insparitions/Considerations Keep DataFrame class clean In order to implement the piping, some of the packages have subclassed, or injected methods to pandas ' DataFrame class, but it may not be necessary. Support of verb calling as regular function Some packages may be too focused on the piping implementation that regular verb calling is also supported in dplyr , as well as piping calling: # legal df %>% select ( a , b ) # legal too select ( df , a , b ) Alignment with dplyr APIs Claiming port of dplyr in python, but not really following dplyr 's API design. Some packages even have their own verbs. This may scare users away as they may have to learn 3 sets of APIs: dplyr , pandas and the one created by the package.","title":"Porting `dplyr` to python: Efforts been made"},{"location":"efforts.html#dplyr-vs-pandas","text":"Why people have made such efforts to port dplyr to python, include me? Well, one of the biggest reasons is that dplyr has much cleaner APIs. pandas has some comparisons between these two with some simple operations on data frames. See its documentation [here]. Check out the left column of the tables, you may find the almost every statement in R / dplyr is shorter. If things go complicated, say, filter a data frame with values from 2 columns: >>> iris [( iris [ 'Species' ] == 'setosa' ) & ( iris [ 'Petal_Length' ] > 1.5 )] Sepal_Length Sepal_Width Petal_Length Petal_Width Species < float64 > < float64 > < float64 > < float64 > < object > 5 5.4 3.9 1.7 0.4 setosa 11 4.8 3.4 1.6 0.2 setosa 18 5.7 3.8 1.7 0.3 setosa 20 5.4 3.4 1.7 0.2 setosa 23 5.1 3.3 1.7 0.5 setosa 24 4.8 3.4 1.9 0.2 setosa 25 5.0 3.0 1.6 0.2 setosa 26 5.0 3.4 1.6 0.4 setosa 29 4.7 3.2 1.6 0.2 setosa 30 4.8 3.1 1.6 0.2 setosa 43 5.0 3.5 1.6 0.6 setosa 44 5.1 3.8 1.9 0.4 setosa 46 5.1 3.8 1.6 0.2 setosa r $> iris %>% filter ( Species == 'setosa' & Petal.Length > 1.5 ) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.4 3.9 1.7 0.4 setosa 2 4.8 3.4 1.6 0.2 setosa 3 5.7 3.8 1.7 0.3 setosa 4 5.4 3.4 1.7 0.2 setosa 5 5.1 3.3 1.7 0.5 setosa 6 4.8 3.4 1.9 0.2 setosa 7 5.0 3.0 1.6 0.2 setosa 8 5.0 3.4 1.6 0.4 setosa 9 4.7 3.2 1.6 0.2 setosa 10 4.8 3.1 1.6 0.2 setosa 11 5.0 3.5 1.6 0.6 setosa 12 5.1 3.8 1.9 0.4 setosa 13 5.1 3.8 1.6 0.2 setosa What if you have even more columns and operations involved? There are some other in-depth comparisons:","title":"dplyr vs pandas"},{"location":"efforts.html#-pythons-pandas-vs-rs-dplyr-which-is-the-best-data-analysis-library","text":"","title":"- Python\u2019s Pandas vs. R\u2019s dplyr \u2013 Which Is The Best Data Analysis Library"},{"location":"efforts.html#efforts-been-made","text":"In order to turn this in R: iris %>% filter ( Species == 'setosa' ) to this in python: iris >> filter ( Species == 'setosa' ) efforts have been made to defer the execution of filter(Species == 'setosa') . Since it gets evaluated when python sees it right away, but we need the data ( iris ) to be piped in to evaluate it.","title":"Efforts been made"},{"location":"efforts.html#suiba","text":"suiba creates a Pipeable object for the verb, and attach __rshift__ method to the DataFrame class or other types. To defer the evaluation of the arguments in the verbs, suiba creates a sui expression that is initiated with a Symbolic object (named _ by default). suiba is powerful and potential to port more related functionalities from R packages, and has great support fro remote tables. It is also attracting attention in the community. suiba has ported a couple verbs from dplyr , including: select() - keep certain columns of data. filter() - keep certain rows of data. mutate() - create or modify an existing column of data. summarize() - reduce one or more columns down to a single number. arrange() - reorder the rows of data. that are most commonly used in data processing. But not all verbs and other helper functions are ported.","title":"suiba"},{"location":"efforts.html#dfply","text":"dfply wraps verbs by a pipe decorator to turn them into a pipeable object (with __rshift__() and __rrshift__() defined). The arguments of the verbs are pre-compiled as Intention objects. Later on the data is piped in and the Intention objects get evaluated.","title":"dfply"},{"location":"efforts.html#plydata","text":"plydata has very similar ideas as dfply does. It uses metaclass to make verbs pipeable. Instead of pre-compile the arguments into objects, plydata uses raw strings as the arguments. The magic for evaluation is to capture the environment from call stacks, by EvalEnvironment.capture() .","title":"plydata"},{"location":"efforts.html#dplython","text":"dplython subclasses pandas ' DataFrame class to DplyFrame , which enables piping ( __rshift__ ) and some grouping functions by itself to easily implement dplyr 's group_by and related functions. The Later objects have carried the information for the function to be evaluated later. The wrapper around it DelayedFunction enables external functions to be registered as verbs.","title":"dplython"},{"location":"efforts.html#insparitionsconsiderations","text":"Keep DataFrame class clean In order to implement the piping, some of the packages have subclassed, or injected methods to pandas ' DataFrame class, but it may not be necessary. Support of verb calling as regular function Some packages may be too focused on the piping implementation that regular verb calling is also supported in dplyr , as well as piping calling: # legal df %>% select ( a , b ) # legal too select ( df , a , b ) Alignment with dplyr APIs Claiming port of dplyr in python, but not really following dplyr 's API design. Some packages even have their own verbs. This may scare users away as they may have to learn 3 sets of APIs: dplyr , pandas and the one created by the package.","title":"Insparitions/Considerations"},{"location":"piping.html","text":"Piping Based on the [survey] we had previously, to port dplyr to python, given df >> select ( ... ) we need to turn this select(...) into select(df, ...) . But select(...) gets executed before python knows the data df on the left side of the piping sign ( >> ). Instead of letting python evaluate the real call of the verb (with the data), we could let select(...) returns an object that holds the arguments and other related information that are needed for the real call to be executed. The execution should happen right after the data is piped in. This could be implemented by method __rrshift__(self, data) , where we could put the data as the first argument of the verb and then evaluate it. The following example implements the idea: class Verb : \"\"\"Works as a decorator to turn verb functions as Verb objects\"\"\" def __init__ ( self , func ): self . func = func self . args = self . kwargs = None def __call__ ( self , * args , ** kwargs ): \"\"\"When python sees `select(...)` in `df >> select(...)`\"\"\" self . args = args self . kwargs = kwargs return self def __rrshift__ ( self , data ): \"\"\"When python sees `df >>`\"\"\" # put data as the first argument of func return self . func ( data , * self . args , ** self . kwargs ) @Verb def select ( df , * columns ): \"\"\"Select columns from df\"\"\" return df [ list ( columns )] from datar.datasets import iris iris >> select ( 'Species' ) # Species # <object> # 0 setosa # 1 setosa # 2 setosa # 3 setosa # .. ... # 4 setosa # 145 virginica # 146 virginica # 147 virginica # 148 virginica # 149 virginica # [150 rows x 1 columns] Normal calling But can do we normal calling as dplyr does: select ( iris , 'Species' ) # <__main__.Verb at 0x7f6f5676d5d0> # but expect the Species series The problem is that this only triggers __call__() but not __rrshift__() . A solution is to trigger __rrshift__() inside __call__() in this situation. But we definitely don't want it to be triggered twice in df >> select(...) . Then how do we know if there is df >> before the call inside select(...) ? There is a way. As when python executes select(...) , the source code has already been written. So we can look up the AST tree to see if there is a >> ( BinOp/RShift ) node: import ast import sys from executing import Source def is_piping (): # need to skip this function frame = sys . _getframe ( 2 ) # executing is a package to accurately detect nodes node = Source . executing ( frame ) . node parent = getattr ( node , 'parent' , None ) return ( parent and isinstance ( parent , ast . BinOp ) and isinstance ( parent . op , ast . RShift ) ) The use it in Verb.__call__() : def __call__ ( self , * args , ** kwargs ): if is_piping (): # do the piping self . args = args self . kwargs = kwargs return self # do the normal call return args [ 0 ] >> self ( * args [ 1 :], ** kwargs ) Now both iris >> select('Species') and select(iris, 'Species') work.","title":"Porting `dplyr` to python: Implementing piping"},{"location":"piping.html#piping","text":"Based on the [survey] we had previously, to port dplyr to python, given df >> select ( ... ) we need to turn this select(...) into select(df, ...) . But select(...) gets executed before python knows the data df on the left side of the piping sign ( >> ). Instead of letting python evaluate the real call of the verb (with the data), we could let select(...) returns an object that holds the arguments and other related information that are needed for the real call to be executed. The execution should happen right after the data is piped in. This could be implemented by method __rrshift__(self, data) , where we could put the data as the first argument of the verb and then evaluate it. The following example implements the idea: class Verb : \"\"\"Works as a decorator to turn verb functions as Verb objects\"\"\" def __init__ ( self , func ): self . func = func self . args = self . kwargs = None def __call__ ( self , * args , ** kwargs ): \"\"\"When python sees `select(...)` in `df >> select(...)`\"\"\" self . args = args self . kwargs = kwargs return self def __rrshift__ ( self , data ): \"\"\"When python sees `df >>`\"\"\" # put data as the first argument of func return self . func ( data , * self . args , ** self . kwargs ) @Verb def select ( df , * columns ): \"\"\"Select columns from df\"\"\" return df [ list ( columns )] from datar.datasets import iris iris >> select ( 'Species' ) # Species # <object> # 0 setosa # 1 setosa # 2 setosa # 3 setosa # .. ... # 4 setosa # 145 virginica # 146 virginica # 147 virginica # 148 virginica # 149 virginica # [150 rows x 1 columns]","title":"Piping"},{"location":"piping.html#normal-calling","text":"But can do we normal calling as dplyr does: select ( iris , 'Species' ) # <__main__.Verb at 0x7f6f5676d5d0> # but expect the Species series The problem is that this only triggers __call__() but not __rrshift__() . A solution is to trigger __rrshift__() inside __call__() in this situation. But we definitely don't want it to be triggered twice in df >> select(...) . Then how do we know if there is df >> before the call inside select(...) ? There is a way. As when python executes select(...) , the source code has already been written. So we can look up the AST tree to see if there is a >> ( BinOp/RShift ) node: import ast import sys from executing import Source def is_piping (): # need to skip this function frame = sys . _getframe ( 2 ) # executing is a package to accurately detect nodes node = Source . executing ( frame ) . node parent = getattr ( node , 'parent' , None ) return ( parent and isinstance ( parent , ast . BinOp ) and isinstance ( parent . op , ast . RShift ) ) The use it in Verb.__call__() : def __call__ ( self , * args , ** kwargs ): if is_piping (): # do the piping self . args = args self . kwargs = kwargs return self # do the normal call return args [ 0 ] >> self ( * args [ 1 :], ** kwargs ) Now both iris >> select('Species') and select(iris, 'Species') work.","title":"Normal calling"},{"location":"showing_dtypes.html","text":"One of the features that I like about dplyr / tidyr is that the data types are always visible when you print a data frame/tibble out: > library ( tidyr ) > table1 # A tibble: 6 x 4 country year cases population < chr > < int > < int > < int > 1 Afghanistan 1999 745 19987071 2 Afghanistan 2000 2666 20595360 3 Brazil 1999 37737 172006362 4 Brazil 2000 80488 174504898 5 China 1999 212258 1272915272 6 China 2000 213766 1280428583 But in python (pandas), you only see this: >>> from datar.all import options >>> from datar.datasets import table1 >>> options ( frame_format_patch = False ) >>> table1 country year cases population 0 Afghanistan 1999 745 19987071 1 Afghanistan 2000 2666 20595360 2 Brazil 1999 37737 172006362 3 Brazil 2000 80488 174504898 4 China 1999 212258 1272915272 5 China 2000 213766 1280428583 If you want to see the data types: >>> table1 . dtypes country object year int64 cases int64 population int64 dtype : object Then how can we display those dtypes under the column names just as dplyr / tidyr does for tibbles? Here is the expected result: >>> options ( frame_format_patch = True ) >>> table1 country year cases population < object > < int64 > < int64 > < int64 > 0 Afghanistan 1999 745 19987071 1 Afghanistan 2000 2666 20595360 2 Brazil 1999 37737 172006362 3 Brazil 2000 80488 174504898 4 China 1999 212258 1272915272 5 China 2000 213766 1280428583 It looks like this when a data frame is shown in Jupyter Notebook: It's sometimes confusing if the data types are invisible. For example: >>> table1 [ 'year' ] = table1 [ 'year' ] . astype ( str ) >>> table1 [ 'cases' ] = table1 [ 'cases' ] . astype ( str ) >>> table1 [ 'sum' ] = table1 [ 'year' ] + table1 [ 'cases' ] >>> table1 [ 'sum_expected' ] = table1 [ 'year' ] . astype ( int ) + table1 [ 'cases' ] . astype ( int ) >>> table1 country year cases population sum sum_expected < object > < object > < object > < int64 > < object > < int64 > 0 Afghanistan 1999 745 19987071 1999745 2744 1 Afghanistan 2000 2666 20595360 20002666 4666 2 Brazil 1999 37737 172006362 199937737 39736 3 Brazil 2000 80488 174504898 200080488 82488 4 China 1999 212258 1272915272 1999212258 214257 5 China 2000 213766 1280428583 2000213766 215766 In such an intended case, we clearly know why the numbers are not adding up, as they are not any of the numeric types. To implement this, attaching the dtypes to the column names was the inital thought. However, when it comes complicated when the column names have multiple levels ( MultiIndex object). So then the idea became to insert the dtypes to the first row of the values. Digging into the source code of pandas DataFrameFormatter , the format_col is a good place to inject the dtypes: def format_col(self, i: int) -> List[str]: \"\"\"Format column, add dtype ahead\"\"\" frame = self.tr_frame formatter = self._get_formatter(i) + dtype = frame.iloc[:, i].dtype.name + return [f'<{dtype}>'] + format_array( - return format_array( frame.iloc[:, i]._values, formatter, float_format=self.float_format, na_rep=self.na_rep, space=self.col_space.get(frame.columns[i]), decimal=self.decimal, leading_space=self.index, ) Then, for the index, we also need to inject a place holder on the same row of the dtypes: def get_strcols(self) -> List[List[str]]: \"\"\" Render a DataFrame to a list of columns (as lists of strings). \"\"\" strcols = self._get_strcols_without_index() if self.index: # dtype + str_index = [\"\"] + self._get_formatted_index(self.tr_frame) - str_index = self._get_formatted_index(self.tr_frame) strcols.insert(0, str_index) return strcols For HTMLFormatter , we also need to increase the nrows of a data frame by 1 for the dtypes, for both regular and hierarchical rows, add some styles for the first row, where the dtypes are showing. Fortunately, the write_tr(...) method allows us to pass tags , which will be added to the HTML tags later on. So we can set the following tags for the dtype rows: style = \"font-style: italic;\" This will make the dtypes show in italic.","title":"Showing data types under column names when printing pandas data frames"},{"location":"verb_args.html","text":"Function calls as verb arguments What if we want to do: iris >> mutate ( mean_sepal_length = mean ( f . Sepal_Length )) f.Sepal_Length and f.Petal_Length refer to two columns in iris . We need to defer the evaluation of the arguments, too, as we did for the verbs. To retrieve the series in the data, we need to let f.Sepal_Length to be evaluated when a data frame is available: class Symbolic : def __getattr__ ( self , name ): self . name = name return self def _eval ( self , data ): return data [ self . name ] Similarly, when python sees mean(f.Sepal_Length) , it gets evaluated. But python hasn't seen the data yet. So the expression needs to be evaluated later, inside Verb.__rrshift__() . This needs us to turn the function mean into Verb -like, but not exactly a Verb , because a verb takes data as the first argument but the function may or may not. Ask the verb to evaluate the arguments with the data: class Verb : # ... # Evaluate args and kwargs in verb def __rrshift__ ( self , data ): args = eval_args ( args , data ) kwargs = eval_kwargs ( kwargs , data ) return self . func ( data , * args , ** kwargs ) eval_args = lambda args , data : ( arg . _eval ( data ) if isinstance ( arg , ( Symbolic , Func )) else arg for arg in args ) eval_kwargs = lambda kwargs , data : { key : ( val . _eval ( data ) if isinstance ( val , ( Symbolic , Func )) else arg ) for key , val in kwargs . items () } Define the function: class Func(Verb): def _eval(self, data): # evaluate my args/kwargs too args = eval_args(args, data) kwargs = eval_kwargs(kwargs, data) return self.func(*args, **kwargs) @Func def mean(series): return series.mean() Operators as verb arguments Operators in python are actually functions, too. For example: import operator a = b = 1 a + b # 2 operator . add ( a , b ) # 2 Then we are able to turn the operators into function in: class Symbolic : def __add__ ( self , other ): return Func ( operator . add )( self , other )","title":"Porting `dplyr` to python: Verb argument evaluation"},{"location":"verb_args.html#function-calls-as-verb-arguments","text":"What if we want to do: iris >> mutate ( mean_sepal_length = mean ( f . Sepal_Length )) f.Sepal_Length and f.Petal_Length refer to two columns in iris . We need to defer the evaluation of the arguments, too, as we did for the verbs. To retrieve the series in the data, we need to let f.Sepal_Length to be evaluated when a data frame is available: class Symbolic : def __getattr__ ( self , name ): self . name = name return self def _eval ( self , data ): return data [ self . name ] Similarly, when python sees mean(f.Sepal_Length) , it gets evaluated. But python hasn't seen the data yet. So the expression needs to be evaluated later, inside Verb.__rrshift__() . This needs us to turn the function mean into Verb -like, but not exactly a Verb , because a verb takes data as the first argument but the function may or may not. Ask the verb to evaluate the arguments with the data: class Verb : # ... # Evaluate args and kwargs in verb def __rrshift__ ( self , data ): args = eval_args ( args , data ) kwargs = eval_kwargs ( kwargs , data ) return self . func ( data , * args , ** kwargs ) eval_args = lambda args , data : ( arg . _eval ( data ) if isinstance ( arg , ( Symbolic , Func )) else arg for arg in args ) eval_kwargs = lambda kwargs , data : { key : ( val . _eval ( data ) if isinstance ( val , ( Symbolic , Func )) else arg ) for key , val in kwargs . items () } Define the function: class Func(Verb): def _eval(self, data): # evaluate my args/kwargs too args = eval_args(args, data) kwargs = eval_kwargs(kwargs, data) return self.func(*args, **kwargs) @Func def mean(series): return series.mean()","title":"Function calls as verb arguments"},{"location":"verb_args.html#operators-as-verb-arguments","text":"Operators in python are actually functions, too. For example: import operator a = b = 1 a + b # 2 operator . add ( a , b ) # 2 Then we are able to turn the operators into function in: class Symbolic : def __add__ ( self , other ): return Func ( operator . add )( self , other )","title":"Operators as verb arguments"}]}